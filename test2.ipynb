{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MH4510 - Statistical Learning and Date Mining Project\n",
    "## Facial Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "packages imported\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "#os.chdir(r'C:\\Users\\NTU user\\Desktop\\MAEC\\Year 3\\MH4510 Data Mining\\MH4510 Face Recognition')\n",
    "os.chdir(r'/Users/alson/Downloads/MH4510 Face Recognition-2/')\n",
    "\n",
    "from full import *\n",
    "from extract_face_v2 import *\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, ZeroPadding2D, Activation, Input, concatenate\n",
    "from keras.models import Model\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.pooling import MaxPooling2D, AveragePooling2D\n",
    "from keras.layers.merge import Concatenate\n",
    "from keras.layers.core import Lambda, Flatten, Dense\n",
    "from keras.initializers import glorot_uniform\n",
    "from keras.engine.topology import Layer\n",
    "from keras import backend as K\n",
    "K.set_image_data_format('channels_first')\n",
    "import cv2\n",
    "#import opencv\n",
    "import numpy as np\n",
    "from numpy import genfromtxt\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from fr_utils import *\n",
    "from inception_blocks_v2 import *\n",
    "from matplotlib.pyplot import imshow\n",
    "from PIL import Image\n",
    "import collections\n",
    "from keras.models import load_model\n",
    "import timeit\n",
    "\n",
    "print(\"packages imported\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-test dataset preparation\n",
    "First, we prepare our train and test datasets. We also load the pre-loaded model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights loaded\n",
      "14.932437483003014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/keras/engine/saving.py:292: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n"
     ]
    }
   ],
   "source": [
    "#%% Predefined things\n",
    "\n",
    "start_time = timeit.default_timer()\n",
    "# code you want to evaluate\n",
    "gpu_memory_fraction = 1.0\n",
    "minsize = 50 # minimum size of face\n",
    "threshold = [ 0.6, 0.7, 0.7 ]  # three steps's threshold\n",
    "factor = 0.709 # scale factor\n",
    "FRmodel = faceRecoModel(input_shape=(3, 96, 96))\n",
    "FRmodel = load_model('face-rec_Google.h5')\n",
    "#%%\n",
    "name = 'th'\n",
    "#image_dir_raw = r'images_raw'+'//'+name\n",
    "#image_dir_rotated = r'images_rotated' +'\\\\' + name\n",
    "#image_dir_bounded= r'images_bounded' +'\\\\' + name\n",
    "image_dir_raw = r'images_raw'+'/'+name\n",
    "image_dir_rotated = r'images_rotated' +'/' + name\n",
    "image_dir_bounded= r'images_bounded' +'/' + name\n",
    "\n",
    "\n",
    "\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "print(\"weights loaded\")\n",
    "print(elapsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dictionary loaded\n"
     ]
    }
   ],
   "source": [
    "name_dict=dict()\n",
    "\n",
    "name_dict[1] = 'alson'\n",
    "name_dict[2] = 'th'\n",
    "name_dict[3] = 'hy'\n",
    "#name_dict[4] = 'wf'\n",
    "#name_dict[5] = 'jace'\n",
    "#name_dict[6] = 'kelly'\n",
    "name_dict[99] = 'unable to identify'\n",
    "print(\"dictionary loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating networks and loading parameters\n"
     ]
    }
   ],
   "source": [
    "#%%\n",
    "\n",
    "#rotate_images(image_dir_raw, image_dir_rotated)\n",
    "#print(os.listdir(image_dir))\n",
    "#%%\n",
    "#import cv2\n",
    "bounding_boxes_compiled = get_bounding_box_coord(image_dir_rotated)\n",
    "\n",
    "#%%\n",
    "bounding(image_dir_rotated, image_dir_bounded, bounding_boxes_compiled, name)\n",
    "\n",
    "#%%\n",
    "encoded_array = encode_img(image_dir_bounded, FRmodel, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_stack(name, label):\n",
    "    #image_dir_raw = r'images_raw'+'//'+name\n",
    "    #image_dir_rotated = r'images_rotated' +'\\\\' + name\n",
    "    #image_dir_bounded= r'images_bounded' +'\\\\' + name\n",
    "    image_dir_raw = r'images_raw'+'/'+name\n",
    "    image_dir_rotated = r'images_rotated' +'/' + name\n",
    "    image_dir_bounded= r'images_bounded' +'/' + name\n",
    "    #rotate_images(image_dir_raw, image_dir_rotated)\n",
    "    #bounding_boxes_compiled = get_bounding_box_coord(image_dir_rotated)\n",
    "    #bounding(image_dir_rotated, image_dir_bounded, bounding_boxes_compiled, name)\n",
    "    encoded_array = encode_img(image_dir_bounded, FRmodel, label)\n",
    "    \n",
    "    return encoded_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "encoded_array_alson = full_stack('alson',1)\n",
    "encoded_array_th = full_stack('th', 2)\n",
    "encoded_array_hy = full_stack('hy', 3)\n",
    "#encoded_array_wf = full_stack('wf',4)\n",
    "#encoded_array_jace = full_stack('jace', 5)\n",
    "#encoded_array_kelly = full_stack('kelly', 6)\n",
    "#%%\n",
    "encoded_array_test = full_stack('test',99)\n",
    "test = encoded_array_test[:,:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_answer = np.array([os.listdir('images_bounded/test')]).T\n",
    "\n",
    "#%%\n",
    "train = encoded_array_alson\n",
    "train = np.vstack([train, encoded_array_th])\n",
    "train = np.vstack([train, encoded_array_hy])\n",
    "#train = np.vstack([train, encoded_array_wf])\n",
    "#train = np.vstack([train, encoded_array_jace])\n",
    "#train = np.vstack([train, encoded_array_kelly])\n",
    "\n",
    "#%%\n",
    "np.save('train_set_v5', train)\n",
    "np.save('test_set_v5', test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we have our train and test datasets now. Next we proceed to filter the images to detect our faces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndatabase[\"5_Jace_1\"] = img_to_encoding(img_dir + \"jace_bounded0.jpg\", FRmodel)\\ndatabase[\"5_Jace_2\"] = img_to_encoding(img_dir + \"jace_bounded1.jpg\", FRmodel)\\ndatabase[\"5_Jace_3\"] = img_to_encoding(img_dir + \"jace_bounded2.jpg\", FRmodel)\\ndatabase[\"5_Jace_4\"] = img_to_encoding(img_dir + \"jace_bounded3.jpg\", FRmodel)\\ndatabase[\"5_Jace_5\"] = img_to_encoding(img_dir + \"jace_bounded4.jpg\", FRmodel)\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "database = {}\n",
    "#img_dir = \"images_bounded\"+\"\\\\\" +\"database\"+ \"\\\\\"\n",
    "img_dir = \"images_bounded\" + \"/\" + \"database\" + \"/\"\n",
    "database[\"3_Hwee Young_1\"] = img_to_encoding(img_dir + \"hy_bounded0.jpg\", FRmodel)\n",
    "database[\"3_Hwee Young_2\"] = img_to_encoding(img_dir + \"hy_bounded1.jpg\", FRmodel)\n",
    "database[\"3_Hwee Young_3\"] = img_to_encoding(img_dir + \"hy_bounded2.jpg\", FRmodel)\n",
    "database[\"3_Hwee Young_4\"] = img_to_encoding(img_dir + \"hy_bounded3.jpg\", FRmodel)\n",
    "database[\"3_Hwee Young_5\"] = img_to_encoding(img_dir + \"hy_bounded4.jpg\", FRmodel)\n",
    "\n",
    "database[\"2_Tze Hong_1\"] = img_to_encoding(img_dir + \"th_bounded0.jpg\", FRmodel)\n",
    "database[\"2_Tze Hong_2\"] = img_to_encoding(img_dir + \"th_bounded1.jpg\", FRmodel)\n",
    "database[\"2_Tze Hong_3\"] = img_to_encoding(img_dir + \"th_bounded2.jpg\", FRmodel)\n",
    "database[\"2_Tze Hong_4\"] = img_to_encoding(img_dir + \"th_bounded3.jpg\", FRmodel)\n",
    "database[\"2_Tze Hong_5\"] = img_to_encoding(img_dir + \"th_bounded4.jpg\", FRmodel)\n",
    "\n",
    "database[\"1_Alson_1\"] = img_to_encoding(img_dir + \"alson_bounded0.jpg\", FRmodel)\n",
    "database[\"1_Alson_2\"] = img_to_encoding(img_dir + \"alson_bounded1.jpg\", FRmodel)\n",
    "database[\"1_Alson_3\"] = img_to_encoding(img_dir + \"alson_bounded2.jpg\", FRmodel)\n",
    "database[\"1_Alson_4\"] = img_to_encoding(img_dir + \"alson_bounded3.jpg\", FRmodel)\n",
    "database[\"1_Alson_5\"] = img_to_encoding(img_dir + \"alson_bounded4.jpg\", FRmodel)\n",
    "\n",
    "'''\n",
    "database[\"5_Jace_1\"] = img_to_encoding(img_dir + \"jace_bounded0.jpg\", FRmodel)\n",
    "database[\"5_Jace_2\"] = img_to_encoding(img_dir + \"jace_bounded1.jpg\", FRmodel)\n",
    "database[\"5_Jace_3\"] = img_to_encoding(img_dir + \"jace_bounded2.jpg\", FRmodel)\n",
    "database[\"5_Jace_4\"] = img_to_encoding(img_dir + \"jace_bounded3.jpg\", FRmodel)\n",
    "database[\"5_Jace_5\"] = img_to_encoding(img_dir + \"jace_bounded4.jpg\", FRmodel)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def who_is_it(image_path, database, model):\n",
    "    \"\"\"\n",
    "    Implements face recognition for the happy house by finding who is the person on the image_path image.\n",
    "    \n",
    "    Arguments:\n",
    "    image_path -- path to an image\n",
    "    database -- database containing image encodings along with the name of the person on the image\n",
    "    model -- your Inception model instance in Keras\n",
    "    \n",
    "    Returns:\n",
    "    min_dist -- the minimum distance between image_path encoding and the encodings from the database\n",
    "    identity -- string, the name prediction for the person on image_path\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ### \n",
    "\n",
    "    ## Step 1: Compute the target \"encoding\" for the image. Use img_to_encoding() see example above. ## (≈ 1 line)\n",
    "    encoding = img_to_encoding(image_path, model)\n",
    "\n",
    "    ## Step 2: Find the closest encoding ##\n",
    "    \n",
    "    # Initialize \"min_dist\" to a large value, say 100 (≈1 line)\n",
    "    min_dist = 100\n",
    "    \n",
    "    # Loop over the database dictionary's names and encodings.\n",
    "    for (name, db_enc) in database.items():\n",
    "        \n",
    "        # Compute L2 distance between the target \"encoding\" and the current \"emb\" from the database. (≈ 1 line)\n",
    "        dist = np.linalg.norm(encoding-db_enc)\n",
    "\n",
    "        # If this distance is less than the min_dist, then set min_dist to dist, and identity to name. (≈ 3 lines)\n",
    "        if dist < min_dist:\n",
    "            min_dist = dist\n",
    "            identity = name\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    if min_dist > 0.045:\n",
    "        print(\"Not in the database.\" +'...'+', the distance is '+str(min_dist) +'...'+image_path)\n",
    "        return False, identity, min_dist, encoding\n",
    "    else:\n",
    "        print (\"it's \" + str(identity) + \", the distance is \" + str(min_dist) +'...'+image_path)\n",
    "        return True, identity, min_dist, encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it's 3_Hwee Young_1, the distance is 0.034381956...images_bounded/test/hy1.jpg\n",
      "it's 1_Alson_2, the distance is 0.039687414...images_bounded/test/a1.jpg\n",
      "it's 1_Alson_2, the distance is 0.043278016...images_bounded/test/a3.jpg\n",
      "it's 3_Hwee Young_1, the distance is 0.033293147...images_bounded/test/hy2.jpg\n",
      "it's 3_Hwee Young_1, the distance is 0.037531532...images_bounded/test/hy3.jpg\n",
      "it's 1_Alson_2, the distance is 0.04443448...images_bounded/test/a2.jpg\n",
      "it's 1_Alson_3, the distance is 0.04199431...images_bounded/test/a5.jpg\n",
      "it's 3_Hwee Young_1, the distance is 0.034296326...images_bounded/test/hy4.jpg\n",
      "it's 3_Hwee Young_5, the distance is 0.032635316...images_bounded/test/hy5.jpg\n",
      "it's 1_Alson_2, the distance is 0.04418828...images_bounded/test/a4.jpg\n",
      "Not in the database...., the distance is 0.05247798...images_bounded/test/k1.jpg\n",
      "Not in the database...., the distance is 0.049345538...images_bounded/test/k3.jpg\n",
      "Not in the database...., the distance is 0.046895195...images_bounded/test/k2.jpg\n",
      "Not in the database...., the distance is 0.048246827...images_bounded/test/k5.jpg\n",
      "Not in the database...., the distance is 0.05043177...images_bounded/test/k4.jpg\n",
      "it's 2_Tze Hong_3, the distance is 0.027053658...images_bounded/test/th4.jpg\n",
      "it's 2_Tze Hong_3, the distance is 0.02288762...images_bounded/test/th5.jpg\n",
      "it's 2_Tze Hong_4, the distance is 0.0048202304...images_bounded/test/th1.jpg\n",
      "it's 2_Tze Hong_3, the distance is 0.01732055...images_bounded/test/th2.jpg\n",
      "it's 2_Tze Hong_3, the distance is 0.022014787...images_bounded/test/th3.jpg\n"
     ]
    }
   ],
   "source": [
    "#img_dir = \"images_bounded\"+\"\\\\\" +\"test\"+ \"\\\\\"\n",
    "img_dir = \"images_bounded\"+\"/\" +\"test\"+ \"/\"\n",
    "images = os.listdir(img_dir)\n",
    "for i in images:\n",
    "    who_is_it(img_dir + i, database, FRmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:757: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='sqrt', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=1000, n_jobs=None,\n",
       "            oob_score=True, random_state=None, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "#import numpy as np\n",
    "#%%\n",
    "train = np.load('train_set_v5.npy')\n",
    "test = np.load('test_set_v5.npy')\n",
    "X = train[:,:-1]\n",
    "Y = train[:,128]\n",
    "#%%\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=10, stratify=Y)\n",
    "\n",
    "#%%\n",
    "\n",
    "model_lr = LogisticRegressionCV(cv = 10, solver='lbfgs', multi_class='multinomial').fit(X_train, y_train)\n",
    "model_rf = RandomForestClassifier(n_estimators=1000, criterion ='gini', max_features = 'sqrt', bootstrap  = True, oob_score  = True)\n",
    "\n",
    "model_rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it's 3_Hwee Young_1, the distance is 0.034381956...images_bounded/test/hy1.jpg\n",
      "it's 1_Alson_2, the distance is 0.039687414...images_bounded/test/a1.jpg\n",
      "it's 1_Alson_2, the distance is 0.043278016...images_bounded/test/a3.jpg\n",
      "it's 3_Hwee Young_1, the distance is 0.033293147...images_bounded/test/hy2.jpg\n",
      "it's 3_Hwee Young_1, the distance is 0.037531532...images_bounded/test/hy3.jpg\n",
      "it's 1_Alson_2, the distance is 0.04443448...images_bounded/test/a2.jpg\n",
      "it's 1_Alson_3, the distance is 0.04199431...images_bounded/test/a5.jpg\n",
      "it's 3_Hwee Young_1, the distance is 0.034296326...images_bounded/test/hy4.jpg\n",
      "it's 3_Hwee Young_5, the distance is 0.032635316...images_bounded/test/hy5.jpg\n",
      "it's 1_Alson_2, the distance is 0.04418828...images_bounded/test/a4.jpg\n",
      "Not in the database...., the distance is 0.05247798...images_bounded/test/k1.jpg\n",
      "Not in the database...., the distance is 0.049345538...images_bounded/test/k3.jpg\n",
      "Not in the database...., the distance is 0.046895195...images_bounded/test/k2.jpg\n",
      "Not in the database...., the distance is 0.048246827...images_bounded/test/k5.jpg\n",
      "Not in the database...., the distance is 0.05043177...images_bounded/test/k4.jpg\n",
      "it's 2_Tze Hong_3, the distance is 0.027053658...images_bounded/test/th4.jpg\n",
      "it's 2_Tze Hong_3, the distance is 0.02288762...images_bounded/test/th5.jpg\n",
      "it's 2_Tze Hong_4, the distance is 0.0048202304...images_bounded/test/th1.jpg\n",
      "it's 2_Tze Hong_3, the distance is 0.01732055...images_bounded/test/th2.jpg\n",
      "it's 2_Tze Hong_3, the distance is 0.022014787...images_bounded/test/th3.jpg\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['hy', 'alson', 'alson', 'hy', 'hy', 'alson', 'alson', 'hy', 'hy',\n",
       "       'alson', 'unable to identify', 'unable to identify',\n",
       "       'unable to identify', 'unable to identify', 'unable to identify',\n",
       "       'th', 'th', 'th', 'th', 'th'], dtype='<U18')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#%%\n",
    "#img_dir = \"images_bounded\"+\"\\\\\" +\"test\"+ \"\\\\\"\n",
    "img_dir = \"images_bounded\"+\"/\" +\"test\"+ \"/\"\n",
    "images = os.listdir(img_dir)\n",
    "\n",
    "final = np.empty([0,5])\n",
    "\n",
    "N = len(images)\n",
    "for i in range(N):\n",
    "    final_row = np.array([])\n",
    "    check, identity_filterlayer, min_dist, encoding = who_is_it(img_dir + images[i], database, FRmodel)\n",
    "    identity_filterlayer = int(identity_filterlayer[0])\n",
    "    #identity_filterlayer = name_dict[identity_filterlayer]\n",
    "    final_row = np.append(final_row, check)\n",
    "    final_row = np.append(final_row, identity_filterlayer)\n",
    "    \n",
    "    if check == True:\n",
    "        final_row = np.append(final_row, model_lr.predict(encoding))\n",
    "        final_row = np.append(final_row, model_rf.predict(encoding))\n",
    "        final_row = np.append(final_row, 0)\n",
    "    \n",
    "    else:\n",
    "        final_row = np.append(final_row, 99)\n",
    "        final_row = np.append(final_row, 99)\n",
    "        final_row = np.append(final_row, 0)\n",
    "        \n",
    "    final = np.vstack([final, final_row])\n",
    "\n",
    "num_results = final.shape[0]\n",
    "#%%\n",
    "for i in range(num_results):\n",
    "    voting_check = np.unique(final[i][1:4], return_counts= True)\n",
    "    voting_idx = np.argmax(voting_check[1])\n",
    "    final[i][4] = voting_check[0][voting_idx]\n",
    "    \n",
    "#%%\n",
    "final_namelist=np.array([name_dict[i] for i in list(final[:,4])])\n",
    "final_namelist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images 0 is hy\n",
      "images 1 is alson\n",
      "images 2 is alson\n",
      "images 3 is hy\n",
      "images 4 is hy\n",
      "images 5 is alson\n",
      "images 6 is alson\n",
      "images 7 is hy\n",
      "images 8 is hy\n",
      "images 9 is alson\n",
      "images 10 is unable to identify\n",
      "images 11 is unable to identify\n",
      "images 12 is unable to identify\n",
      "images 13 is unable to identify\n",
      "images 14 is unable to identify\n",
      "images 15 is th\n",
      "images 16 is th\n",
      "images 17 is th\n",
      "images 18 is th\n",
      "images 19 is th\n"
     ]
    }
   ],
   "source": [
    "#print(final_namelist[0])\n",
    "#final_namelist.size\n",
    "#np.ptp(final_namelist,1)\n",
    "#for i in np.ptp(final_namelist, axis=1):\n",
    "N=len(final_namelist)\n",
    "for i in range(N):\n",
    "    print(\"images \"+ str(i) + \" is \" +final_namelist[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
